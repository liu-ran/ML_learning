{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI Introduction to Machine Learning\n",
        "**Day 3: Transformers**\n",
        "\n",
        "\n",
        "Notebook adapted by Gage DeZoort from a similar notebook offered in Princeton University's [Introduction to Machine Learning Wintersession course](https://github.com/PrincetonUniversity/intro_machine_learning/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/GageDeZoort/intro_ml_uci/blob/main/day_3/intro_to_ML_day_3_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "zpCkWYWdmgbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Imports"
      ],
      "metadata": {
        "id": "smb8fYeN7ENJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAaLG3PSyc-w",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "!pip install datasets -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F3gjqazyc-x"
      },
      "source": [
        "\n",
        "\n",
        "The goal of this tutorial is to train a sequence-to-sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jbjvb8yc-y"
      },
      "source": [
        "## 1. Tokens for Language Modeling\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1yFgIZpyc-y"
      },
      "source": [
        "Given a word or sequence of words, how likely is some subsequent word? This is a fundamental language modeling task: assigning a likelihood probability to a word to follow some input sequence.\n",
        "\n",
        "\n",
        "As an example, let's consider the following input sequence:\n",
        "\n",
        "*I need to take my dog to the vet because he is*\n",
        "\n",
        "What's the next word? *Hungry*? *Healthy*? *Sick*?\n",
        "\n",
        "You get the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Tokenization\n",
        "\n",
        "Machines need to analyze **tokenized** data. Tokens can be words, phrases, characters, etc. They have corresponding `IDs` that are stored in a lookup table.\n",
        "\n",
        "We're going to use a model called *BERT* (Bidirectional Transformers) as our tokenizer. BERT is a transformer model, whose tokenizer splits the input text into words and punctuation, ignoring whitespace. It also splits complicated words into subwords. See below how the string `\"deeeep\"` which does not appear in the English language, is split into three tokens `['dee', '##ee', '##p']`. The latter two tokens are called *subwords*.\n",
        "\n",
        "Google's propriatary WordPiece algorithm is used to build BERT's vocabulary (of subwords) built iteratively from an initial vocab of single character tokens. Frequent character pairs are merged into new subwords until its 30,000 token vocabulary is constructed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H7xtViWL430Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Choose a pre-trained model tokenizer (e.g., BERT)\n",
        "model_name = \"bert-base-uncased\" # 100M parameters, not case-sensitive\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example: Tokenizing text\n",
        "text = \"Transformers are a type of deeeep learning model used for NLP tasks. Epehmeral. Anachronism.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Converting tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Decoding token IDs back to text\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "id": "ISRStxoX6VIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Sequence Data\n",
        "\n",
        "\n",
        "To create a coherent learning task, we need to take sequences of tokens and batch them into inputs with corresponding targets. Sequences are batched into uniform-length chunks. For example consider two words written as sequences of tokens:\n",
        "\n",
        "Sequence #1: `[\"run\", \"##ner\"]`\n",
        "\n",
        "Sequence #2: `[\"d\", \"##run\", \"#k\", \"##en\"]`\n",
        "\n",
        "Our model will expect fixed-size sequences at input, say of size `max_length=3`. Sequence #1 is shorter than `max_length`, so we have to *pad* it with some default value. In BERT, this default value is `[PAD]`. Sequence #2, on the other hand, is longer than `max_length`, so we have to *truncate* it."
      ],
      "metadata": {
        "id": "UyfvjiUldZhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding and truncation\n",
        "\n",
        "sequence = tokenizer(text, padding=\"max_length\", truncation=True, max_length=10)\n",
        "print(\"Encoded Sequence:\", sequence)\n",
        "tokenizer.decode(sequence[\"input_ids\"])"
      ],
      "metadata": {
        "id": "tyLbb0tedgjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the `input_IDs` are what the BERT transformer will actually process, the `token_type_ids` are used to demarkate segments (for next-sentence prediction), and the `attention_mask` indicates which tokens are padding (0). Note that BERT's tokenizer has added a few special tokens. `[CLS]` is a classification token marking the start of the sequence, and `[SEP]` is the separater token marking the end."
      ],
      "metadata": {
        "id": "C1Wh7pHz6aIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Transformer Models\n",
        "\n",
        "BERT is a pre-trained transformer model available for generic use cases. It takes as input the `sequence` data type we generated above and outputs **embeddings**, or high-dimensional vectors summarizing the tokens *and* their contextual meaning, for each token."
      ],
      "metadata": {
        "id": "dEo-0Scnk89c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load a pre-trained model\n",
        "model_name = \"bert-base-uncased\" # 100M parameters, not case-sensitive\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Example input\n",
        "inputs = tokenizer(\"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\")\n",
        "print(\"Inputs:\", inputs)\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# The model outputs embeddings\n",
        "print(\"Last hidden state shape:\", outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "f8_30kqjgQp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see that each of the 12 words gets a 768 dimensional output embedding. Again, this embedding has taken into account the **context** of each token - its semantic meaning in relation to every other token in the sequence. In the following cells, we'll dig into how this is done."
      ],
      "metadata": {
        "id": "tWGWW7r0nh2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Attention is All You Need\n",
        "\n",
        "Transformers use **attention**, which quantify how much tokens in a sequence focus on other tokens. Attention is a way to add context into the embedding for each token. Attention is implemented as a **dot product**; you can think of it like a **cosine similarity score** between the vectors representing each input token."
      ],
      "metadata": {
        "id": "T2PfMFYNn2vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity** measures how aligned two vectors are — that is, how close they point in direction, regardless of their magnitudes.  \n",
        "\n",
        "Mathematically:\n",
        "$$\n",
        "C_s(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}||\\ ||\\mathbf{y}||}\n",
        "$$\n",
        "\n",
        "- $C_s = 1$ → vectors point the same way (highly similar)\n",
        "- $C_s = 0$ → vectors are orthogonal (unrelated)\n",
        "- $C_s = -1$ → vectors point in opposite directions (opposite meaning)\n",
        "\n",
        "In attention, this score plays the same role as the **dot product** inside the softmax — it tells us *how much one token should pay attention to another*."
      ],
      "metadata": {
        "id": "rwbMcRfMEVnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let's say our inputs are the following words, each with a 5-dimensional vector representation:\n",
        "\n",
        "$$\n",
        "  \\text{Attention}\\rightarrow [2, 1, 0, 1, 1]\n",
        "  \\\\\n",
        "  \\text{Beer}\\rightarrow  [-1, -1, 0, 2, 0]\n",
        "  \\\\\n",
        "  \\text{Need}\\rightarrow [1, 1, 1, 1, 1]\n",
        "$$"
      ],
      "metadata": {
        "id": "sKTWZvWKEcuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Token vectors\n",
        "attention = np.array([2, 1, 0, 1, 1])\n",
        "fame = np.array([-1, -1, 0, 2, 0])\n",
        "need = np.array([1, 1, 1, 1, 1])\n",
        "tokens = {\"Attention\": attention, \"Fame\": fame, \"Need\": need}\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "labels = list(tokens.keys())\n",
        "n = len(labels)\n",
        "cos_sim_matrix = np.zeros((n, n))\n",
        "\n",
        "for i, (t1, v1) in enumerate(tokens.items()):\n",
        "    for j, (t2, v2) in enumerate(tokens.items()):\n",
        "        cos_sim_matrix[i, j] = cosine_similarity(v1, v2)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cos_sim_matrix, annot=True, fmt=\".2f\",\n",
        "    xticklabels=labels, yticklabels=labels,\n",
        "    cmap=\"Blues\", vmin=-1, vmax=1, cbar_kws={\"label\": \"Cosine similarity\"}\n",
        ")\n",
        "plt.title(\"Cosine Similarity Between Word Vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pNw-gM4XFu5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drill down on how attention is actually computed.  \n",
        "\n",
        "In a given layer of a Transformer model, our $N$ tokens—each represented by a $D$-dimensional feature vector—are stored as a matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\n",
        "$$\n",
        "\n",
        "To compute attention, we form **three learned representations** of these tokens:\n",
        "\n",
        "$$\n",
        "\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q,\n",
        "\\quad\n",
        "\\mathbf{K} = \\mathbf{X}\\mathbf{W}_K,\n",
        "\\quad\n",
        "\\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{D \\times D}$ are learnable parameter matrices  \n",
        "- $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ denote the resulting **queries**, **keys**, and **values**\n",
        "\n",
        "---\n",
        "\n",
        "The attention mechanism asks: **how similar are the queries and the keys?**  \n",
        "We answer this by computing a **dot product** between every query and every key:\n",
        "\n",
        "$$\n",
        "\\mathbf{Q}\\mathbf{K}^\\top =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{q}_1 \\!\\cdot\\! \\mathbf{k}_1 & \\mathbf{q}_1 \\!\\cdot\\! \\mathbf{k}_2 & \\cdots & \\mathbf{q}_1 \\!\\cdot\\! \\mathbf{k}_N \\\\\n",
        "\\mathbf{q}_2 \\!\\cdot\\! \\mathbf{k}_1 & \\mathbf{q}_2 \\!\\cdot\\! \\mathbf{k}_2 & \\cdots & \\mathbf{q}_2 \\!\\cdot\\! \\mathbf{k}_N \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\mathbf{q}_N \\!\\cdot\\! \\mathbf{k}_1 & \\mathbf{q}_N \\!\\cdot\\! \\mathbf{k}_2 & \\cdots & \\mathbf{q}_N \\!\\cdot\\! \\mathbf{k}_N\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This matrix simply stores the **pairwise similarities** between all queries and keys.  \n",
        "Importantly, these representations are *learned* — the model discovers the most useful notion of “similarity” for the task at hand.\n",
        "\n",
        "---\n",
        "\n",
        "To compute attention in full, we normalize each row of this similarity matrix with a softmax:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(\\mathbf{Q}, \\mathbf{K})\n",
        "= \\texttt{Softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{D}}\\right)\n",
        "$$\n",
        "\n",
        "The factor of $1/\\sqrt{D}$ normalizes the magnitude of the dot products, keeping the **temperature** of the softmax stable (since softmax is highly sensitive to input scale).\n",
        "\n",
        "---\n",
        "\n",
        "Each row of this matrix sums to 1.  \n",
        "The entry in row $i$, column $j$ represents **how much token $i$ attends to token $j$** — in other words, how much information token $i$ gathers from token $j$.\n",
        "\n",
        "Note that this matrix is **not symmetric**: token $j$ may give a very different amount of attention to token $i$.\n",
        "\n",
        "---\n",
        "\n",
        "**Steps:**\n",
        "1. Compute pairwise similarities: $QK^\\top$  \n",
        "   → how aligned each query is with each key  \n",
        "2. Scale by $\\sqrt{D}$ to stabilize gradients  \n",
        "3. Apply $\\text{softmax}$ to get attention weights (probabilities that sum to 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7MI8MXZ_EY2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 1**: Fill in the missing code in the cell below to compute an attention matrix. Verify that its rows sum to 1."
      ],
      "metadata": {
        "id": "gTmVuvAfRbbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# Toy example: 4 tokens, 3D features\n",
        "# -----------------------------\n",
        "torch.manual_seed(0)\n",
        "N, D = 4, 3  # 4 tokens, each 3-dimensional\n",
        "X = torch.randn(N, D)\n",
        "print(\"Input tokens (X):\\n\", X, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Learned projection matrices (for demonstration only)\n",
        "# -----------------------------\n",
        "W_Q = torch.randn(D, D)\n",
        "W_K = torch.randn(D, D)\n",
        "W_V = torch.randn(D, D)\n",
        "\n",
        "# Compute Queries, Keys, and Values\n",
        "# Q = TO DO\n",
        "# K = TO DO\n",
        "# V = TO DO\n",
        "\n",
        "print(\"Q:\\n\", Q, \"\\n\")\n",
        "print(\"K:\\n\", K, \"\\n\")\n",
        "print(\"V:\\n\", V, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Compute similarity scores\n",
        "# -----------------------------\n",
        "# scores = TO DO\n",
        "print(\"Raw dot-product similarities (QK^T):\\n\", scores, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Scale by sqrt(D)\n",
        "# -----------------------------\n",
        "# norm_factor = TO DO\n",
        "scaled_scores = scores / norm_factor\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Apply softmax row-wise\n",
        "# -----------------------------\n",
        "attn_weights = F.softmax(scaled_scores, dim=-1)\n",
        "print(\"Attention weights (softmax over rows):\\n\", attn_weights, \"\\n\")\n",
        "\n",
        "## VERIFY THAT ROWS SUM TO 1"
      ],
      "metadata": {
        "id": "EpzPcRK6ETri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "1. Compute pairwise similarities: $QK^\\top$  \n",
        "   → how aligned each query is with each key  \n",
        "2. Scale by $\\sqrt{D}$ to stabilize gradients  \n",
        "3. Apply $\\text{softmax}$ to get attention weights (probabilities that sum to 1)\n",
        "\n",
        "...now what do we do with attention scores?\n",
        "\n",
        "4. **Use those weights to take a weighted sum of the value vectors $V$**\n",
        "\n",
        "$$\n",
        "  \\text{Output} = \\text{Attention}(\\mathbf{Q}, \\mathbf{K}) \\times \\mathbf{V}\n",
        "$$\n",
        "\n",
        "To be clear, the attention matrix tells us how much we should weight the corresponding entries of the **value** matrix."
      ],
      "metadata": {
        "id": "0dOrFgsyKfOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 4) Compute attention output\n",
        "# -----------------------------\n",
        "attn_output = attn_weights @ V\n",
        "print(\"Attention output (weighted sum of V):\\n\", attn_output)"
      ],
      "metadata": {
        "id": "LcKO5QWEO-SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In Transformers:\n",
        "- Each token’s embedding generates its own $Q$, $K$, and $V$.  \n",
        "- The attention scores decide *how much information to pull* from each other token.  \n",
        "- **Multi-head attention** repeats this process in parallel with multiple sets of $(W_Q, W_K, W_V)$ to capture diverse relationships, then concatenates and linearly projects the results.\n"
      ],
      "metadata": {
        "id": "5IHA22huO7X3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine-tuning\n",
        "\n",
        "We've got pre-trained models like BERT available to us. These models have been trained on massive corpora and have excellent general language capabilities. Fine tuning is the process of tuning a pre-trained model, which is a much more efficient approach than re-tuning a language model from scratch."
      ],
      "metadata": {
        "id": "R0KaKCMv8qFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "!pip install evaluate\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "rvg_2g-i6u5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**DistilBERT** is a smaller, faster version of BERT trained through **knowledge distillation** — where a large **teacher** model (BERT) teaches a smaller **student** model to mimic its behavior.\n",
        "\n",
        "- ~40% fewer parameters, ~60% faster inference  \n",
        "- Retains ~97% of BERT’s performance on NLP benchmarks  \n",
        "- Uses **6 Transformer layers** (vs. 12 in BERT-base)  \n",
        "\n",
        "DistilBERT is trained to perform **masked language modeling** (like BERT); mask random words in a sentence, ask the model to predict them."
      ],
      "metadata": {
        "id": "_B_Bcep2BRzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\" # \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification"
      ],
      "metadata": {
        "id": "kQB6L3x36hK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎬 The IMDb Dataset\n",
        "\n",
        "The [IMDb dataset](https://huggingface.co/datasets/stanfordnlp/imdb) contains **50,000 movie reviews**, each labeled for **sentiment analysis** —  \n",
        "`0` for *negative* and `1` for *positive* sentiment.\n",
        "\n",
        "These reviews are preprocessed as sequences of tokens and used to train models to perform **sentiment analysis**, that is to **detect emotional tone** present in language.\n",
        "\n",
        "For example, what label do you think this review would have?\n",
        "\n",
        "> *“National Treasure is about as over-rated and over-hyped as they come. Nicholas Cage is in no way a believable action hero, and this film is no 'Indiana Jones'. People who have compared this movie to the Indiana Jones classic trilogy have seriously fallen off their rocker...”*\n",
        "\n",
        "Pretty clearly **negative**, right?  \n",
        "That’s exactly the kind of reasoning a sentiment classifier learns to reproduce."
      ],
      "metadata": {
        "id": "WtG6oU3XBV2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "-S3P8SUF9F0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a small fraction of the dataset (e.g., 10%)\n",
        "fraction = 0.1\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]) * fraction)))\n",
        "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]) * fraction)))\n",
        "\n",
        "# Verify the size\n",
        "print(f\"Train size: {len(small_train_dataset)}, Test size: {len(small_test_dataset)}\")\n",
        "\n",
        "# Tokenize the smaller datasets\n",
        "def preprocess_data(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "small_train_dataset = small_train_dataset.map(preprocess_data, batched=True)\n",
        "small_test_dataset = small_test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Convert to PyTorch format\n",
        "small_train_dataset = small_train_dataset.rename_column(\"label\", \"labels\")\n",
        "small_test_dataset = small_test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "small_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "small_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "uIRHqsw1_s37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to move tensors to the correct device (GPU/CPU)\n",
        "def move_to_device(batch):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Move tensor columns to the correct device\n",
        "    batch = {key: value.to(device) if torch.is_tensor(value) else value for key, value in batch.items()}\n",
        "    return batch\n",
        "\n",
        "# Apply this function to your dataset using `map`\n",
        "small_train_dataset = small_train_dataset.map(move_to_device, batched=True)\n",
        "small_test_dataset = small_test_dataset.map(move_to_device, batched=True)"
      ],
      "metadata": {
        "id": "jP-HyFoHIi7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Model device:\", next(model.parameters()).device)  # This should print \"cuda\" if using GPU"
      ],
      "metadata": {
        "id": "BTg7qVj4IvCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = predictions.argmax(axis=-1)  # Get the class with the highest probability\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=10,\n",
        "    report_to=None,\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision if on GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ycHfWO3L9GIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "TyH_4UjF9O4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Solutions to Exercises"
      ],
      "metadata": {
        "id": "bcEoAWeqR80j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 1**\n",
        "\n",
        "Simply run the cell below."
      ],
      "metadata": {
        "id": "p4Qr1CL0SBy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# Toy example: 4 tokens, 3D features\n",
        "# -----------------------------\n",
        "torch.manual_seed(0)\n",
        "N, D = 4, 3  # 4 tokens, each 3-dimensional\n",
        "X = torch.randn(N, D)\n",
        "print(\"Input tokens (X):\\n\", X, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Learned projection matrices (for demonstration only)\n",
        "# -----------------------------\n",
        "W_Q = torch.randn(D, D)\n",
        "W_K = torch.randn(D, D)\n",
        "W_V = torch.randn(D, D)\n",
        "\n",
        "# Compute Queries, Keys, and Values\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "print(\"Q:\\n\", Q, \"\\n\")\n",
        "print(\"K:\\n\", K, \"\\n\")\n",
        "print(\"V:\\n\", V, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Compute similarity scores\n",
        "# -----------------------------\n",
        "scores = Q @ K.T\n",
        "print(\"Raw dot-product similarities (QK^T):\\n\", scores, \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Scale by sqrt(D)\n",
        "# -----------------------------\n",
        "scaled_scores = scores / torch.sqrt(torch.tensor(D, dtype=torch.float32))\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Apply softmax row-wise\n",
        "# -----------------------------\n",
        "attn_weights = F.softmax(scaled_scores, dim=-1)\n",
        "print(\"Attention weights (softmax over rows):\\n\", attn_weights, \"\\n\")"
      ],
      "metadata": {
        "id": "mtgrjTBGSC3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "277VDQwfSDTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "smb8fYeN7ENJ",
        "B-jbjvb8yc-y",
        "dEo-0Scnk89c",
        "R0KaKCMv8qFU",
        "bcEoAWeqR80j"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}